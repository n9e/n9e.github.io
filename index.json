[{"RelPermalink":"/blog/say-hello-to-doks/","contents":"","description":"Introducing Doks, a Hugo theme helping you build modern documentation websites that are secure, fast, and SEO-ready â€” by default.","title":"Say hello to Doks ðŸ‘‹"},{"RelPermalink":"/docs/usage/datasource/","contents":"Nightingale supports various data sources, including:\n Prometheus, as well as other storage systems that support the Prometheus protocol, such as VictoriaMetrics and Thanos ElasticSearch, as well as other storage systems that support the ElasticSearch protocol, such as OpenSearch Grafana Loki  Add a data source in the Integrations-Data sources, select the corresponding data source type, fill in the data source address, username, password, and other information, and click Save.\n","description":null,"title":"Config datasource"},{"RelPermalink":"/docs/agent/intro/","contents":"Nightingale is an alarm engine, which does not need to be integrated with the collector, but directly connects to various data sources for query alarms.\nThat is to say, if you have collected various monitoring data and stored it in the time series database, you can configure the time series database as a data source in Nightingale, and Nightingale can directly query the data in the time series database. There is no need to use various collectors mentioned in this chapter.\nHowever, many new users have not built their own collection capabilities, so we provide some collector docking solutions to facilitate users to get started quickly. However, Nightingale still does not provide storage capabilities. These collectors collect data and push it to Nightingale, and Nightingale then forwards the data to the time series database.\nIn the Nightingale configuration file etc/config.toml, there is a [[Pushgw.Writers]] section, which is used to configure the address of the time series database. After receiving the data, Nightingale forwards the data to these addresses.\n","description":"","title":"Pre explanation"},{"RelPermalink":"/docs/prologue/introduction/","contents":"Nightingale is an open-source project focused on alerting. Similar to Grafana\u0026rsquo;s data source integration approach, Nightingale also integrates with various existing data sources. While Grafana focuses on visualization, Nightingale focuses on alerting.\nNightingale can query data from multiple data sources, generate alarm events, and then send alerts via various notification channels. In addition, we also have an event pipeline design, which can perform different processing on alarm events, such as filtering, relabeling, enriching, and so on.\nRepo #  Backend: https://github.com/ccfos/nightingale Frontend: https://github.com/n9e/fe  Any issues or PRs are welcome!\nWorking logic # Many users have already collected metrics and log data themselves. In this case, they can integrate their storage repositories (like VictoriaMetrics, Elasticsearch, etc.) as data sources in Nightingale. Users can then configure alert rules and notification rules in Nightingale to generate and dispatch alert events.\nNightingale itself does not provide data collection capabilities. We recommend using Categraf as a collector, which can seamlessly integrate with Nightingale.\nCategraf can collect monitoring data from operating systems, network devices, middleware, and databases. It pushes this data to Nightingale (via Prometheus Remote Write protocol), which then forwards the data to time-series databases (like Prometheus, VictoriaMetrics, etc.) and provides alerting and visualization capabilities.\nFor specific edge data centers, where the network link to the central Nightingale server is poor, Nightingale also provides a design for edge data center alerting engine deployment. In this mode, even if the edge and central networks are disconnected, alerting functionality remains unaffected.\n In the above diagram, the network link between data center A and the central data center is good, so the alerting engine is handled by the central Nightingale process. For data center B, where the network link to the central data center is poor, we deploy n9e-edge as the alerting engine to handle data source alerting functionality locally.\n Alerting, Upgrades, and Collaboration # Nightingale focuses on being an alerting engine, responsible for generating alert events and flexibly dispatching them based on rules. It has built-in support for 20 notification channels (like phone calls, SMS, email, DingTalk, Feishu, WeCom, Slack, etc.).\nIf you have more advanced requirements, such as:\n Want to aggregate events from multiple monitoring systems into one platform for unified noise reduction, response handling, and data analysis Want to support team on-call culture, including features like alert claim, escalation (to avoid missing alerts), and collaborative handling  Then Nightingale may not be suitable. We recommend using FlashDuty, an on-call product that aggregates alerts from various monitoring systems for unified noise reduction, distribution, and response.\nKey Capabilities #  Nightingale supports alert rules, muting rules, subscription rules, and notification rules. It natively integrates 20 notification channels and allows customization of message templates. Nightingale supports event pipelines to process alert events and integrate with third-party systems. For example, it can perform operations like relabeling, filtering, and enriching on events. Nightingale supports the concept of business groups and introduces a permission system to manage various rules in a categorized manner. Many databases and middleware have built-in alert rules that can be directly imported for use, and Prometheus alert rules can also be directly imported. Nightingale supports alert self-healing, which means that after an alert is triggered, a script is automatically executed to perform some predefined logic, such as cleaning up the disk or capturing the on-site situation.   Nightingale archives historical alert events and supports multi-dimensional querying and statistics. It allows flexible aggregation and grouping, making it easy to get a clear overview of the distribution of the company\u0026rsquo;s alert events at a glance.   Nightingale has built-in metric explanations, dashboards, and alert rules for common operating systems, middleware, and databases. However, these are all contributed by the community, and their overall quality varies. Nightingale directly receives data from multiple protocols such as Remote Write, OpenTSDB, Datadog, and Falcon, thus enabling integration with various types of agents. Nightingale supports multiple data sources including Prometheus, ElasticSearch, Loki, and TDEngine, and can perform alerting based on the data from them. Nightingale can be easily embedded into internal enterprise systems, such as Grafana and CMDB. It even allows configuring the menu visibility of these embedded systems.   Nightingale supports dashboard functionality, featuring common chart types and some built-in dashboards. The image above is a screenshot of one of these dashboards. If you\u0026rsquo;re already accustomed to Grafana, it\u0026rsquo;s recommended to continue using Grafana for viewing charts, as Grafana has more profound expertise in this area. For machine-related monitoring data collected by Categraf, it\u0026rsquo;s advisable to use Nightingale\u0026rsquo;s built-in dashboards. This is because Categraf\u0026rsquo;s metric naming follows Telegraf\u0026rsquo;s naming convention, which differs from that of Node Exporter. Since Nightingale incorporates the concept of business groups, where machines can belong to different business groups, there are times when you may only want to view machines belonging to the current business group in the dashboard. Therefore, Nightingale\u0026rsquo;s dashboards can be linked with business groups.  Thank you for the trust from numerous enterprises. # Nightingale has been adopted by many enterprises, including but not limited to:\nOpen Source License # The Nightingale monitoring project is open-sourced under the Apache License 2.0.\n","description":"Nightingale is an open-source project focused on alerting. Similar to Grafana's data source integration approach, Nightingale also connects with various existing data sources. However, while Grafana focuses on visualization, Nightingale emphasizes alerting engines.","title":"Introduction"},{"RelPermalink":"/docs/usage/ad-hoc/","contents":"Nightingale supports ad-hoc queries, which allow you to query data sources directly on the interface. You can query metric data in the Metrics-Explorer menu and log data in the Log Analysis-Explorer menu.\n","description":null,"title":"Ad-hoc Query"},{"RelPermalink":"/docs/usage/alert-rules/","contents":"Nightingale supports metric alerts and log alerts. According to the alert rules configured by the user, the data source is queried periodically. When the data in the data source meets the rule threshold, the alert is triggered.\nYou can manually create alert rules, import built-in alert rules, or import Prometheus alert rules.\n","description":null,"title":"Config alert rules"},{"RelPermalink":"/docs/prologue/prometheus/","contents":"Nightingale is similar to Grafana in that it can integrate with a variety of data sources, the most common of which is Prometheus-type. Other data sources that are compatible with the Prometheus interface, such as VictoriaMetrics, Thanos, and M3DB, can also be considered Prometheus-type sources, so the relationship between the two is close.\nIf you have the following requirements, you might consider using Nightingale:\n You have multiple time-series databases, such as Prometheus and VictoriaMetrics, and want to use a unified platform to manage various alert rules with permission control. You are concerned about the single point of failure of Prometheus\u0026rsquo;s alerting engine and want to avoid downtime. In addition to Prometheus alerts, you need alerts from other data sources such as ElasticSearch, Loki, and ClickHouse. You require more flexible alert rule configurations, such as controlling the effective time, event relabeling, event linkage with CMDB, and supporting alert self-healing scripts.  Nightingale also has visualization capabilities similar to Grafana, but it may not be as advanced. In my observation, many companies adopt a combination approach (in the adult world, there are no absolutes):\n Data Collection: A combination of various agents and exporters is used, with Categraf being the primary choice (especially for machine monitoring, seamlessly integrated with Nightingale), supplemented by various exporters. Storage: The time-series database primarily used is VictoriaMetrics, as it is compatible with Prometheus, offers better performance, and has a clustered version. For most companies, the single-node version is sufficient. Alerting Engine: Nightingale is used for alerting, making it easy for different teams to manage and collaborate. It comes with some built-in rules out of the box, and the configuration of alert rules is very flexible, with an event pipeline mechanism that facilitates integration with their own CMDB, etc. Visualization: Grafana is used for visualization, as it offers more advanced and visually appealing charts. The community is also very large, and many pre-made dashboards can be found on the Grafana site, making it relatively hassle-free. On-call Distribution of Alert Events: FlashDuty is used, which supports integration with various monitoring systems such as Zabbix, Prometheus, Nightingale, cloud monitoring solutions, Elastalert, etc. It consolidates alert events into a single platform for unified noise reduction, scheduling, claim escalation, response, distribution, and more.  ","description":"Nightingale and Prometheus are often discussed in relation to each other, and in fact, they have a complementary relationship. This article will detail the differences and connections between the two.","title":"Nightingale vs Prometheus"},{"RelPermalink":"/docs/prologue/pre-knowledge/","contents":"Nightingale monitoring (Nightingale) is part of the Prometheus ecosystem, so many concepts and knowledge of Prometheus are prerequisites for using Nightingale. This article lists the key knowledge and provides relevant learning materials, hoping to be helpful to you.\nBasic Knowledge #  Linux knowledge, such as process-related, network-related, systemd-related, etc. Prometheus knowledge, which is essential for using Nightingale. You can refer to the Prometheus official documentation.  ","description":"Nightingale monitoring (Nightingale) is an open-source monitoring system. This article introduces some basic knowledge and concepts that need to be understood before learning Nightingale. Monitoring-related knowledge is very complex, and I hope readers can be patient.","title":"Prior knowledge"},{"RelPermalink":"/docs/prologue/architecture/","contents":"Nightingale\u0026rsquo;s architecture is relatively simple. For testing purposes, a single binary can be used to start it. However, for production environments, it relies on MySQL and Redis. Some companies have multiple data centers, and some edge data centers have poor network quality; Nightingale has been specifically designed to address such scenarios.\nArchitecture Diagram # If we do not consider the edge mode, Nightingale has only one main process, the n9e process, which relies on MySQL and Redis to store some management data. It can connect to various data sources, and the technical architecture diagram is as follows:\nInitially supported data sources include Prometheus, VictoriaMetrics, and ElasticSearch, which support both visualization and alerting. However, Nightingale is focusing more on the alerting engine, so new data sources will only support alerting.\nBased on whether the monitoring data flows through Nightingale, we can distinguish two modes:\n Mode 1: Monitoring data does not flow through Nightingale. Users handle data collection themselves and only configure the time-series databases in Nightingale for visualization and alerting. The architecture diagram above is a typical example of this mode. Mode 2: Data flows through Nightingale. Categraf pushes data to Nightingale via the remote write protocol. Nightingale does not store data directly but forwards it to time-series databases, which are determined by the Pushgw.Writers configuration in the config.toml file. The architecture diagram for Mode 2 is as follows:  In the diagram above, Nightingale receives monitoring data and forwards it to VictoriaMetrics. It can also forward data to Prometheus; if you want to forward to Prometheus, remember to enable the remote receiver feature when starting Prometheus (you can check which control parameter to use with ./prometheus --help | grep receiver), which enables the /api/v1/write interface in Prometheus.\n ðŸŸ¢ If you are a new user, it is recommended to use VictoriaMetrics directly, as it offers better performance and supports clustered modes. Additionally, it is compatible with the Prometheus interface. However, there is less Chinese documentation available for VictoriaMetrics compared to Prometheus.\n Single Node Testing Mode # To quickly test Nightingale, download the release package from Nightingale GitHub releases. After downloading, extract the package, and you will find a binary file named n9e. You can run it directly with ./n9e, which defaults to port 17000. The default username is root, and the password is root.2020.\nThe n9e process only relies on the etc and integrations directories in the same directory as the binary and does not depend on any other services.\nIn this single-node mode, it is convenient for quick testing but not recommended for production environments. In this mode, Nightingale stores configuration data (such as user information, alert rules, dashboards, etc.) in a local SQLite database file, so after starting the n9e process, a file named n9e.db will be generated in the same directory.\nSingle Node Production Mode # If you want to deploy Nightingale in a production environment, it requires dependencies on MySQL and Redis. Therefore, you need to configure the connection information for MySQL and Redis in the etc/config.toml file.\nMySQL key configuration example:\n[DB] DBType = \u0026quot;mysql\u0026quot; DSN = \u0026quot;root:YourPa55word@tcp(localhost:3306)/n9e_v6?charset=utf8mb4\u0026amp;parseTime=True\u0026amp;loc=Local\u0026quot;  In the above DSN (Data Source Name), the format is username:password@tcp(address:port)/database_name?parameters, where n9e_v6 is the name of the Nightingale database. Since version V6, this name has been consistently used (even though we are now at V8+), and it has been retained in the table creation statements.\nRedis key configuration example:\n[Redis] Address = \u0026quot;127.0.0.1:6379\u0026quot; RedisType = \u0026quot;standalone\u0026quot;  The above is just a basic configuration example. The configuration file contains many other configuration items, which can be referenced in the comments within the file or in the configuration file documentation.\nNightingale Cluster # The cluster mode is straightforward; you just need to set up multiple machines, deploy the n9e process on each machine (the process requires the etc and integrations directories to function properly), and ensure that the configuration files of all n9e processes are identical. They should share the same MySQL and Redis instances to work correctly.\nMultiple n9e processes will automatically distribute alert rules. For example, if there are 2 n9e processes and users have configured a total of 100 alert rules, Nightingale will automatically assign these 100 alert rules to the 2 n9e processes, with each process handling approximately 50 alert rules (each rule will only run on one n9e instance, ensuring no duplicate alerts). If one machine goes down, the n9e process on the other machine will take over its alert rules and continue operating.\nEdge Mode # The several modes mentioned above are all centralized modes. However, in actual production environments, there may be multiple computer rooms, and the network quality between the central computer room and edge computer rooms may be poor. If the n9e in the central computer room is made responsible for alerting on a time-series database in an edge computer room, it will be unstable, and sometimes n9e may even fail to connect directly to the time-series database in the edge computer room. In such cases, Nightingale\u0026rsquo;s edge mode is required.\nAssume your company has 3 data centers: a central primary data center, edge data center A, and edge data center B. A has a dedicated line to the central facility with good network quality, while B uses public network with unreliable connectivity.\nThe n9e process is deployed in the central data center, along with its dependencies MySQL and Redis. For high availability, deploy multiple central n9e instances with identical configurations connecting to the same MySQL and Redis.\nIn the diagram, there are 5 data sources:\n Central data center: Loki and ElasticSearch Edge A: ElasticSearch and Prometheus Edge B: VictoriaMetrics  To uniformly view data from all 5 sources in the central n9e, configure their access addresses in Nightingale via Integrations - Data sources.\nThe central n9e can directly access data sources in the central and edge A facilities via internal addresses but cannot reach edge B\u0026rsquo;s VictoriaMetrics (no dedicated line). Thus, expose edge B\u0026rsquo;s VictoriaMetrics via a public address for central n9e access:\n Expose edge B\u0026rsquo;s VictoriaMetrics at a public address (e.g., https://ex.a.com) Configure this public URL in Nightingale\u0026rsquo;s web UI for edge B\u0026rsquo;s VictoriaMetrics  Lines 1-5 in the diagram represent connections between the central n9e and the 5 data sources. When users query data via Nightingale\u0026rsquo;s web interface, requests are proxied through the n9e process to backend data sources, which return results to the user.\nn9e-edge is deployed in edge B to handle alert evaluations for its VictoriaMetrics. It synchronizes alert rules from the central n9e (line A in the diagram), caches them in memory, and evaluates alerts against the local VictoriaMetrics. This ensures reliable alerting as n9e-edge and VictoriaMetrics are internally connected. Even if n9e-edge loses connectivity to the central n9e, alert evaluation continues using cached rules.\nAlert events generated by n9e-edge are written to the central MySQL via n9e\u0026rsquo;s API and sent to notification services (DingTalk, Feishu, FlashDuty, etc.). If network connectivity between n9e-edge and n9e fails, events cannot be written to MySQL, but notifications can still be sent if the edge facility has external network access.\nIn the architecture:\n Central n9e handles alert evaluation for central Loki/ElasticSearch and edge A\u0026rsquo;s ElasticSearch/Prometheus Edge B\u0026rsquo;s n9e-edge handles alert evaluation for its VictoriaMetrics  To associate data sources with specific alert engines, configure this in the data source management page:\nIn the diagram:\n URL: Address used by central n9e to read data (public address for edge B\u0026rsquo;s VictoriaMetrics in the example) Time-series Database Intranet Address: Address used by n9e-edge to connect to VictoriaMetrics. Leave blank to use the URL if it\u0026rsquo;s already an internal address. For edge B, configure an internal address for reliable alert evaluation. Remote Write URL: VictoriaMetrics\u0026rsquo; remote write address for recording rules. n9e-edge processes recording rules and writes results back to the time-series database, so use an internal address. Optional if recording rules are unused. Associated Alert Engine Cluster: Selected as \u0026ldquo;edge-b\u0026rdquo; (name specified by EngineName in edge.toml), establishing the association between edge B\u0026rsquo;s n9e-edge and its VictoriaMetrics for handling alert/recording rules.  Newer Nightingale versions require n9e-edge to use a dedicated Redis instance in edge B, separate from the central n9e\u0026rsquo;s Redis (labeled R1 and R2 in the diagram).\nFor Categraf: if network connectivity is good (central and edge A), Categraf can report directly to the central n9e. For edge B with n9e-edge, configure Categraf to connect to the local n9e-edge.\nConfiguration Examples # Sample configurations for the above architecture:\nCentral n9e Configuration # The central n9e uses etc/config.toml:\n[HTTP.APIForService] Enable = true [HTTP.APIForService.BasicAuth] user001 = \u0026quot;ccc26da7b9aba533cbb263a36c07dcc5\u0026quot; user002 = \u0026quot;ccc26da7b9aba533cbb263a36c07dcc6\u0026quot;  Key configuration: HTTP.APIForService. Default Enable = false (security measure). Set to true to support n9e-edge. n9e-edge uses BasicAuth for authentication (configured above with two users for demonstration). If n9e is exposed publicly, modify default passwords to prevent attacks.\nEdge n9e-edge Configuration # n9e-edge uses etc/edge/edge.toml. Configure the central n9e address for rule synchronization:\n[CenterApi] Addrs = [\u0026quot;http://N9E-CENTER-SERVER:17000\u0026quot;] BasicAuthUser = \u0026quot;user001\u0026quot; BasicAuthPass = \u0026quot;ccc26da7b9aba533cbb263a36c07dcc5\u0026quot; # unit: ms Timeout = 9000  N9E-CENTER-SERVER:17000 is the central n9e address. BasicAuthUser and BasicAuthPass match central credentials (omit if BasicAuth is disabled). Always modify default passwords for public exposure.\nNew n9e-edge versions require Redis (configure in edge.toml). Check for Redis settings in the default edge.toml to confirm dependency.\nEdge Categraf Configuration # Configure writer and heartbeat addresses to point to n9e-edge:\n... [[writers]] url = \u0026quot;http://N9E-EDGE:19000/prometheus/v1/write\u0026quot; ... [heartbeat] enable = true # report os version cpu.util mem.util metadata url = \u0026quot;http://N9E-EDGE:19000/v1/n9e/heartbeat\u0026quot; ...  N9E-EDGE:19000 is the n9e-edge address (default port 19000, configurable in edge.toml).\nIbex Configuration # For self-healing functionality (ibex), enable in edge.toml:\n[Ibex] Enable = true RPCListen = \u0026quot;0.0.0.0:20090\u0026quot;  Configure edge Categraf to connect to n9e-edge\u0026rsquo;s RPC port:\n[ibex] enable = true ## ibex flush interval interval = \u0026quot;1000ms\u0026quot; ## n9e ibex server rpc address servers = [\u0026quot;N9E-EDGE-IP:20090\u0026quot;] ## temp script dir meta_dir = \u0026quot;./meta\u0026quot;  N9E-EDGE-IP:20090 is the n9e-edge RPC address (no http:// prefix for RPC).\nOther Use Cases # Beyond poor network links, edge mode suits secure network partitions where only a relay machine connects to the central n9e. Deploy n9e-edge on the relay, with other machines\u0026rsquo; Categraf instances connecting to it.\n","description":"This article explains the architecture design of Nightingale monitoring (Nightingale), including the design of centralized clusters and the deployment mode of edge data centers.","title":"Architecture"},{"RelPermalink":"/docs/prologue/flashcat/","contents":"Nightingale\u0026rsquo;s core development team is fully dedicated to open source. Without a sustainable income, the project cannot continue. Therefore, they founded a company: Beijing Flashcat Cloud Technology Co., Ltd., abbreviated as \u0026ldquo;Flashcat\u0026rdquo; and launched two commercial products:\n One-stop Intelligent Observation Platform Flashcat One-stop Alarm Response Platform Flashduty  Among them, Flashcat can be regarded as the commercial version of Nightingale, positioned as a unified observation platform, while the open-source version focuses only on metric monitoring. The differences between the two can be referenced in this article: ã€ŠDifferences Between Nightingale Open Source Version and Commercial Versionã€‹.\nFlashDuty is a one-stop alarm response platform similar to PagerDuty (i.e., On-call center), supporting integration with various monitoring systems such as Zabbix, Prometheus, Nightingale, cloud monitoring, Elastalert, etc. It consolidates alarm events into a single platform and unifies the processing of alarms, including convergence and noise reduction, scheduling, claim escalation, response, dispatch, and more. You can register for a free trial .\nThe knowledge in the monitoring and observability field is indeed very complex. If you have the budget and need third-party assistance, you can consider Flashcat\u0026rsquo;s products for a win-win situation ðŸ‘‰ Free product idea exchange with Flashcat.\n","description":"The monitoring field is indeed very complex. If you need third-party assistance, you can consider the Nightingale commercial version, which offers more features and services.","title":"Enterprise Edition"},{"RelPermalink":"/docs/prologue/videos/","contents":"To help users quickly get started with Nightingale, we have created a series of introductory video tutorials covering various aspects from installation and deployment to basic usage. Here is the list of these videos (it is recommended to watch at double speed for a quick overview):\n Nightingale Project Introduction, Resource Links, etc., Must-Watch Nightingale Architecture Explanation Basic Functionality Introduction Installing Categraf for Nightingale Integration Introduction to Common Categraf Plugins Notification Rules Introduction  ","description":"Nightingale's introductory video tutorials help users quickly get started with Nightingale.","title":"Videos for Getting Started"},{"RelPermalink":"/docs/install/pre-intro/","contents":"Common installation methods include:\n Binary deployment Docker Compose deployment Helm deployment  The recommended method is binary deployment, for the following reasons:\n Nightingale consists of a single binary file with minimal dependencies, making it easier to manage. Most users are familiar with systemd, so you can simply use systemd to manage the Nightingale process. Docker Compose deployment may have slightly lower performance compared to binary deployment, and it requires additional knowledge of Docker. Additionally, issues with pulling images due to network restrictions in China can be troublesome. Helm deployment is suitable for Kubernetes environments, but since monitoring systems are critical (P0 level), if Kubernetes goes down, the monitoring system will also be affected. This can lead to complaints from other teams about the lack of planning.  Regardless of the installation method, after installation, the default username for Nightingale is root, and the password is root.2020. Nightingale listens on port 17000 by default, and in edge mode, the n9e-edge port is 19000.\nIf you are using edge mode, please be sure to read the Edge Mode Documentation section.\n","description":"Nightingale supports various installation methods, including binary deployment, Docker Compose deployment, and Helm deployment. Which one should you choose? This article provides some suggestions.","title":"Pre-Introduction"},{"RelPermalink":"/docs/install/upgrade/","contents":"Nightingale versions V6, V7, and V8 have the same upgrade method between their minor versions.\nUpgrade Steps #  Backup Data: Before upgrading, back up the MySQL database content, binary files, and the etc and integrations directories. Having a backup allows you to operate confidently. If you are using binary deployment, replace the binary files and the integrations directory (you can simply move the old integrations directory to a backup location using mv integrations integrations.bak, and then use the new integrations directory). Compare the new and old configuration files using diff, and manually fill in any differences (in practice, you should rarely need to modify the configuration files, as they have not changed much for a long time). If you are using container deployment, pull the latest image, compare the configuration files, fill in any differences, and then restart the container.  About Database Table Structure # If the database account used by Nightingale has permissions to create and modify tables, you do not need to manually change the database table structure. Nightingale will automatically check if the table structure needs to be upgraded at startup, and if so, it will modify the tables automatically. If the database account does not have permissions to create or modify tables, you will need to make manual adjustments. Recent changes can be found in migrate.sql. If automatic table modification fails, please submit an issue, and we will follow up as soon as possible.\nIn theory, the database supports both MySQL and Postgres, but the community lacks long-term contributors for Postgres, so it is recommended to use MySQL first.\n","description":"Nightingale monitoring (Nightingale) how to upgrade between different versions, what to pay attention to, and which files to replace","title":"Upgrade"},{"RelPermalink":"/docs/install/binary/","contents":"If you have not read the section on Pre-Installation Instructions, please do so before reading this section.\nDownload # Download the latest version from GitHub, and you will get a compressed package similar to n9e-${version}-linux-amd64.tar.gz. This is the release package for the X86 CPU architecture. If you need the ARM architecture, download the arm64 package. There is no Windows version of the release package because Nightingale is a server-side project that typically runs on Linux systems.\nIf you want to run Nightingale on Windows and Mac, it\u0026rsquo;s also OK, but you need to compile it yourself. The compilation is relatively simple, and you can refer to the logic in the Makefile in the project code repository.\nUnzip the downloaded package to the /opt/n9e directory.\nmkdir /opt/n9e \u0026amp;\u0026amp; tar zxvf n9e-${version}-linux-amd64.tar.gz -C /opt/n9e  Single Node Test Installation # In this mode, it is only for testing purposes, and it does not depend on MySQL or Redis (it actually uses SQLite and an in-memory Redis: miniredis). The startup is straightforward; you can start it directly after unzipping.\nStart the Process # cd /opt/n9e \u0026amp;\u0026amp; nohup ./n9e \u0026amp;\u0026gt; n9e.log \u0026amp;  Since this is just a test mode, we directly use nohup to start it. In a production environment, you would typically use systemd to manage the n9e process.\nCheck the Process # # Check if the process is running ss -tlnp | grep 17000  Login # Open your browser and visit http://localhost:17000. The default username is root, and the default password is root.2020.\n Please replace localhost with your server\u0026rsquo;s IP address.\n Single Node Production Installation # In a production environment, we recommend using MySQL and Redis to store data.\nModify Configuration # You need to configure the connection information for MySQL and Redis in the /opt/n9e/etc/config.toml file.\nMySQL key configuration example:\n[DB] DBType = \u0026quot;mysql\u0026quot; DSN = \u0026quot;YourUsername:YourPassword@tcp(127.0.0.1:3306)/n9e_v6?charset=utf8mb4\u0026amp;parseTime=True\u0026amp;loc=Local\u0026quot;  Redis key configuration example:\n[Redis] Address = \u0026quot;127.0.0.1:6379\u0026quot; Password = \u0026quot;YourRedisPassword\u0026quot; RedisType = \u0026quot;standalone\u0026quot;  Start the Process # Start the n9e binary, and Nightingale will automatically create the database tables. This requires your database connection account to have permissions to create and modify tables.\nnohup ./n9e \u0026amp;\u0026gt; n9e.log \u0026amp; # Check if the process started successfully ps -ef | grep n9e ss -tlnp | grep 17000  nohup allows for quick startup verification; if there are any issues, check the n9e.log file.\nUsing systemd for Management # In a production environment, it is recommended to use systemd to manage the n9e process. Create a systemd service file at /etc/systemd/system/n9e.service with the following content:\n[Unit] Description=Nightingale Monitoring Service After=network.target [Service] Type=simple ExecStart=/opt/n9e/n9e WorkingDirectory=/opt/n9e Restart=always RestartSec=5 StandardOutput=syslog StandardError=syslog SyslogIdentifier=n9e [Install] WantedBy=multi-user.target  After saving the file, run the following commands to enable and start the service:\nsudo systemctl enable n9e sudo systemctl start n9e  Login # Open your browser and visit http://localhost:17000. The default username is root, and the default password is root.2020.\n Please replace localhost with your server\u0026rsquo;s IP address.\n Cluster Mode # The logic of the cluster mode has already been explained in Nightingale Architecture Design, so it will not be repeated here. From the deployment perspective, you just need to set up multiple machines, deploy one n9e process on each machine, and configure the connection information for MySQL and Redis properly. Multiple n9e processes share the same set of MySQL and Redis, so the configuration files of these n9e processes are exactly the same.\nEdge Mode # For instructions on the edge mode, please be sure to read this first: Nightingale Monitoring - Detailed Explanation of Edge Alert Engine Architecture!!!\nThe edge mode uses the n9e-edge binary, which can be found in the n9e-${version}-linux-amd64.tar.gz compressed package. n9e-edge needs to communicate with the n9e of the central server to synchronize alert rules, so the configuration file of n9e-edge must include the connection information of the central n9e.\nEdge Cluster # Multiple instances of n9e-edge in edge computer rooms can also be deployed to form a cluster. The configuration files of multiple n9e-edge instances within the same cluster must be consistent. Instances with the same EngineName in the configuration file will be regarded as multiple instances of the same engine cluster, and the name should be different from that of the central n9e. The default EngineName of the central n9e is default, while the default EngineName of the edge n9e-edge is edge.\nIf you have multiple edge computer rooms, the EngineName of n9e-edge in each edge computer room needs to be different, such as edge1, edge2, etc. Only after such differentiation can different data sources be assigned to different alert engines.\nStart n9e-edge # Note that when starting the n9e-edge process, you need to specify the configuration directory instead of the configuration file. For example:\nnohup ./n9e-edge --configs etc/edge \u0026amp;\u0026gt; edge.log \u0026amp;  The etc/edge mentioned above is the configuration directory. It would be incorrect to write it as --configs etc/edge/edge.toml.\n","description":"Deploying Nightingale Monitoring using binary files","title":"Binary Deployment"},{"RelPermalink":"/docs/install/compose/","contents":"Download # Refer to the Binary Installation section to download the Nightingale monitoring release package, which includes Docker Compose configuration files. Alternatively, you can directly download the Nightingale source repository, where you can also find the Docker Compose configuration files.\nStart # Whether you download the release package or the source code repository, there will be a docker/compose-bridge directory after decompression. Simply enter this directory and execute the docker-compose up -d command (due to domestic network restrictions, image downloads may fail, and you need to solve the problem of scientific internet access on your own).\nroot@ubuntu-linux-22-04-desktop:/opt/n9e/docker/compose-bridge# docker compose up -d [+] Running 5/5 âœ” Container victoriametrics Started 0.6s âœ” Container redis Started 0.6s âœ” Container mysql Started 0.6s âœ” Container nightingale Started 0.2s âœ” Container categraf Started 0.2s root@ubuntu-linux-22-04-desktop:/opt/n9e/docker/compose-bridge# docker compose ps NAME IMAGE COMMAND SERVICE CREATED STATUS PORTS categraf m.daocloud.io/docker.io/flashcatcloud/categraf:latest \u0026quot;/entrypoint.sh\u0026quot; categraf 2 minutes ago Up 3 seconds mysql mysql:8 \u0026quot;docker-entrypoint.sâ€¦\u0026quot; mysql 2 minutes ago Up 4 seconds 0.0.0.0:3306-\u0026gt;3306/tcp, :::3306-\u0026gt;3306/tcp, 33060/tcp nightingale m.daocloud.io/docker.io/flashcatcloud/nightingale:latest \u0026quot;sh -c /app/n9e\u0026quot; nightingale 2 minutes ago Up 3 seconds 0.0.0.0:17000-\u0026gt;17000/tcp, :::17000-\u0026gt;17000/tcp, 0.0.0.0:20090-\u0026gt;20090/tcp, :::20090-\u0026gt;20090/tcp redis redis:6.2 \u0026quot;docker-entrypoint.sâ€¦\u0026quot; redis 2 minutes ago Up 4 seconds 0.0.0.0:6379-\u0026gt;6379/tcp, :::6379-\u0026gt;6379/tcp victoriametrics victoriametrics/victoria-metrics:v1.79.12 \u0026quot;/victoria-metrics-pâ€¦\u0026quot; victoriametrics 2 minutes ago Up 4 seconds 0.0.0.0:8428-\u0026gt;8428/tcp, :::8428-\u0026gt;8428/tcp  Docker Compose starts multiple containers, which are:\n victoriametrics: Time-series database, compatible with Prometheus, with better performance. redis: Cache database, Nightingale uses Redis to store JWT tokens and machine heartbeat metadata mysql: Relational database, Nightingale uses MySQL to store user information, alert rules, dashboards, and other configuration data. nightingale: The core service of Nightingale monitoring. categraf: Monitoring data collector, responsible for collecting CPU, memory, disk, and other metric data from the host.  Login # Use your browser to visit http://localhost:17000 to open the Nightingale monitoring page. The default username is root, and the default password is root.2020.\n Please replace localhost with your server\u0026rsquo;s IP address.\n Cluster Mode # In cluster mode, multiple n9e instances need to share the same MySQL and Redis, so you cannot simply use the default Docker Compose. You need to modify the config.toml file in the etc-nightingale directory to configure the unified MySQL and Redis connection information.\nEdge Mode # The edge mode requires the n9e-edge process. However, the community does not provide a Docker image for n9e-edge, so in edge mode, it is still necessary to deploy the n9e-edge process using the binary method. For detailed instructions on the edge mode, please refer to: Nightingale Monitoring - Detailed Explanation of Edge Alert Engine Architecture.\n","description":"Nightingale monitoring (Nightingale) supports Docker Compose deployment, this article introduces how to deploy Nightingale using Docker Compose.","title":"Docker Compose"},{"RelPermalink":"/docs/install/helm/","contents":"You can run Nightingale using the n9e helm chart to deploy it in a Kubernetes cluster.\nThe default username for Nightingale is root, and the password is root.2020.\nHowever, we do not recommend deploying Nightingale in Kubernetes because the monitoring system is too critical. If the Kubernetes cluster encounters issues, it may cause the monitoring system to malfunction. At that time, you might want to use the monitoring data to troubleshoot Kubernetes issues, leading to a circular dependency. Especially when other teams find that they cannot use the monitoring system, they may come to you with complaints.\n","description":"Use Helm chart to install Nightingale monitoring (Nightingale) and deploy it in Kubernetes.","title":"Helm"},{"RelPermalink":"/docs/install/configuration/","contents":"The configuration file for the central n9e is etc/config.toml, and the configuration file for the edge alert engine n9e-edge is etc/edge/edge.toml. Here we first explain the n9e configuration file in sections.\nGlobal # [Global] RunMode = \u0026quot;release\u0026quot;  This is a configuration item used by Nightingale developers; ordinary users don\u0026rsquo;t need to care about it. It should always remain release.\nLog # [Log] # stdout, stderr, file Output = \u0026quot;stdout\u0026quot; # log write dir Dir = \u0026quot;logs\u0026quot; # log level: DEBUG INFO WARNING ERROR Level = \u0026quot;DEBUG\u0026quot; # # rotate by time # KeepHours = 4 # # rotate by size # RotateNum = 3 # # unit: MB # RotateSize = 256   Output: Log output method, supporting stdout, stderr, file. Only in file mode will logs be output to files, and the following configuration items will be used. Dir: Directory for storing log files Level: Log level, supporting DEBUG, INFO, WARNING, ERROR KeepHours: Log file retention time in hours. Logs can be rotated by time or size. If rotated by time, use this configuration item (one log file per hour); if rotated by size, use the following two configuration items. RotateNum: Number of retained log files RotateSize: Log file size in MB  HTTP # [HTTP] # http listening address Host = \u0026quot;0.0.0.0\u0026quot; # http listening port Port = 17000 # https cert file path CertFile = \u0026quot;\u0026quot; # https key file path KeyFile = \u0026quot;\u0026quot; # whether print access log PrintAccessLog = false # whether enable pprof PProf = true # expose prometheus /metrics? ExposeMetrics = true # http graceful shutdown timeout, unit: s ShutdownTimeout = 30 # max content length: 64M MaxContentLength = 67108864 # http server read timeout, unit: s ReadTimeout = 20 # http server write timeout, unit: s WriteTimeout = 40 # http server idle timeout, unit: s IdleTimeout = 120   Host: HTTP service listening address, usually 0.0.0.0 to listen on all network interfaces Port: HTTP service listening port CertFile: HTTPS certificate file path KeyFile: HTTPS key file path PrintAccessLog: Whether to print access logs PProf: Whether to enable pprof. If enabled, pprof information can be viewed at /api/debug/pprof/ ExposeMetrics: Whether to expose Prometheus\u0026rsquo;s /metrics interface for exposing Nightingale\u0026rsquo;s own monitoring metrics ShutdownTimeout: HTTP service graceful shutdown timeout in seconds MaxContentLength: Maximum HTTP request length in bytes ReadTimeout: HTTP read timeout in seconds WriteTimeout: HTTP write timeout in seconds IdleTimeout: HTTP idle timeout in seconds  HTTP.ShowCaptcha # [HTTP.ShowCaptcha] Enable = false   Enable: Whether to enable the captcha function  HTTP.APIForAgent # [HTTP.APIForAgent] Enable = true # [HTTP.APIForAgent.BasicAuth] # user001 = \u0026quot;ccc26da7b9aba533cbb263a36c07dcc5\u0026quot;   Enable: Whether to enable the API interface for Agent. Normally, this must be enabled, so this configuration item is generally true. BasicAuth: The Agent\u0026rsquo;s API interface supports BasicAuth. Configure BasicAuth username and password here. For internal network communication, BasicAuth is not required; for public network communication, it is recommended to configure BasicAuth, and the password must not use the default one to avoid attacks. In the example above, user001 is the BasicAuth username, and ccc26da7b9aba533cbb263a36c07dcc5 is the BasicAuth password. To configure multiple users, you can add more entries, for example:  [HTTP.APIForAgent.BasicAuth] user001 = \u0026quot;ccc26da7b9aba533cbb263a36c07dcc5\u0026quot; user002 = \u0026quot;d4f5e6a7b8c9d0e1f2g3h4i5j6k7l8m9\u0026quot;  Note: If you configure BasicAuth, the corresponding username and password must also be configured in the Agent\u0026rsquo;s n9e configuration file; otherwise, the Agent cannot connect to the central n9e.\nThe default configuration has Enable set to true and HTTP.APIForAgent.BasicAuth empty, indicating that the API interfaces for Agent are enabled without BasicAuth.\nHTTP.APIForService # [HTTP.APIForService] Enable = false [HTTP.APIForService.BasicAuth] user001 = \u0026quot;ccc26da7b9aba533cbb263a36c07dcc5\u0026quot;   Enable: Whether to enable the API interface for Service. The edge alert engine n9e-edge communicates with the central n9e through these interfaces. So if you use n9e-edge, this needs to be enabled (set to true). BasicAuth: The Service\u0026rsquo;s API interface supports BasicAuth. Configure BasicAuth username and password here. For internal network communication, BasicAuth is not required; for public network communication, it is recommended to configure BasicAuth, and the password must not use the default one to avoid attacks. In the example above, user001 is the BasicAuth username, and ccc26da7b9aba533cbb263a36c07dcc5 is the BasicAuth password. To configure multiple users, you can add more entries, for example:  [HTTP.APIForService.BasicAuth] user001 = \u0026quot;ccc26da7b9aba533cbb263a36c07dcc5\u0026quot; user002 = \u0026quot;d4f5e6a7b8c9d0e1f2g3h4i5j6k7l8m9\u0026quot;  Note: If you configure BasicAuth, the corresponding username and password must also be configured in the n9e-edge configuration file; otherwise, n9e-edge cannot connect to the central n9e. The default configuration has Enable set to false, meaning the API interfaces for other Services are not enabled, and n9e-edge cannot connect to the central n9e.\nHTTP.JWTAuth # [HTTP.JWTAuth] # unit: min AccessExpired = 1500 # unit: min RefreshExpired = 10080 RedisKeyPrefix = \u0026quot;/jwt/\u0026quot;  Nightingale uses JWT for authentication. Here configure JWT expiration times in minutes: AccessExpired is the expiration time of the access token, and RefreshExpired is the expiration time of the refresh token. For the roles of these two tokens in JWT mechanism, you can ask GPT for details, which will not be elaborated here. Nightingale stores some JWT-related information in Redis, and RedisKeyPrefix is the prefix for Redis keys, which generally does not need to be changed.\nHTTP.ProxyAuth # [HTTP.ProxyAuth] # if proxy auth enabled, jwt auth is disabled Enable = false # username key in http proxy header HeaderUserNameKey = \u0026quot;X-User-Name\u0026quot; DefaultRoles = [\u0026quot;Standard\u0026quot;]  If you want to embed Nightingale into your own system, you can consider using ProxyAuth, similar to Grafana\u0026rsquo;s ProxyAuth. It means that after a user logs in to your system, you can get the username and put it in the X-User-Name header to pass to Nightingale, and Nightingale will consider the user logged in. DefaultRoles is the default role; if you don\u0026rsquo;t pass roles, Nightingale will treat the user as having the Standard role.\nIn fact, according to observations, no community users are currently using this function, so please use it with caution.\nHTTP.RSA # [HTTP.RSA] OpenRSA = false  When logging in to Nightingale, user passwords are transmitted in plain text. If the Nightingale site uses HTTPS, it\u0026rsquo;s fine; if it\u0026rsquo;s HTTP, it\u0026rsquo;s recommended to enable RSA encryption to prevent plain text transmission of user passwords.\nDB # [DB] # mysql postgres sqlite DBType = \u0026quot;sqlite\u0026quot; # postgres: host=%s port=%s user=%s dbname=%s password=%s sslmode=%s # postgres: DSN=\u0026quot;host=127.0.0.1 port=5432 user=root dbname=n9e_v6 password=1234 sslmode=disable\u0026quot; # mysql: DSN=\u0026quot;root:1234@tcp(localhost:3306)/n9e_v6?charset=utf8mb4\u0026amp;parseTime=True\u0026amp;loc=Local\u0026quot; DSN = \u0026quot;n9e.db\u0026quot; # enable debug mode or not Debug = false # unit: s MaxLifetime = 7200 # max open connections MaxOpenConns = 32 # max idle connections MaxIdleConns = 8  DBType and DSN are the most critical, and the two configurations are linked. DBType supports three databases: mysql, postgres, and sqlite. DSN is the database connection information; for sqlite, it\u0026rsquo;s the database file path; for mysql or postgres, it\u0026rsquo;s the database connection information.\nStarting from version v8, Nightingale sets DBType to sqlite by default to facilitate users\u0026rsquo; quick experience without installing a database. However, in production environments, please use mysql or postgres.\nFor DSN configurations of Postgres and MySQL, you can refer to the examples in the comments. Other configurations are related to database connections and can be modified according to your environment. For general small and medium-sized environments, setting MaxOpenConns to 32 and MaxIdleConns to 8 is sufficient.\nRedis # [Redis] # standalone cluster sentinel miniredis RedisType = \u0026quot;miniredis\u0026quot; # address, ip:port or ip1:port,ip2:port for cluster and sentinel(SentinelAddrs) Address = \u0026quot;127.0.0.1:6379\u0026quot; # Username = \u0026quot;\u0026quot; # Password = \u0026quot;\u0026quot; # DB = 0 # UseTLS = false # TLSMinVersion = \u0026quot;1.2\u0026quot; # Mastername for sentinel type # MasterName = \u0026quot;mymaster\u0026quot; # SentinelUsername = \u0026quot;\u0026quot; # SentinelPassword = \u0026quot;\u0026quot;  Redis is used not only to store JWT-related login authentication information but also to store metadata reported by machine heartbeats. The machine offline alert rules supported in Nightingale judge based on the heartbeat time of machines in Redis. If there is no heartbeat for a long time, the machine is considered offline.\n If Redis responds slowly, it may cause false judgments of offline alerts. That is, the machine is actually alive, but the heartbeat information in Redis is not updated in time, eventually leading Nightingale to mistakenly judge the machine as offline. Starting from version V8.beta11, monitoring indicators related to Redis operations have been added. These indicators need to be paid attention to to detect slow Redis response issues in time.\n RedisType supports four types: standalone, cluster, sentinel, and miniredis. Starting from Nightingale v8, Nightingale uses miniredis by default to facilitate users\u0026rsquo; quick experience without installing Redis. However, in production environments, please use other modes.\nAddress is the Redis connection address, and the configuration method varies according to RedisType:\n standalone: When RedisType is standalone, Address is the address of the Redis instance in the format ip:port cluster: When RedisType is cluster, Address is the address of the Redis cluster in the format ip1:port,ip2:port sentinel: When RedisType is sentinel, Address is the address of Redis Sentinel in the format ip1:port,ip2:port. In sentinel mode, MasterName, SentinelUsername, and SentinelPassword also need to be configured UseTLS: Whether to use TLS TLSMinVersion: Minimum TLS version, which takes effect only when UseTLS is true  Alert # Starting from a certain version, Nightingale merged the webapi and alert engine modules to reduce deployment complexity. The configuration items here under Alert are for the alert engine.\nAlert.Heartbeat # [Alert.Heartbeat] # auto detect if blank IP = \u0026quot;\u0026quot; # unit ms Interval = 1000 EngineName = \u0026quot;default\u0026quot;   IP: The IP address of the alert engine. If empty, Nightingale will automatically detect it. Each alert engine writes heartbeat information to MySQL, so that each alert engine knows the list of all alive alert engines, and then can perform sharding processing of alert rules. For example, with 100 alert rules and a cluster of two n9e instances, each n9e will process approximately 50 rules. When one alert engine goes down, the other will take over all 100 rules. Interval: Heartbeat interval in milliseconds EngineName: The name of the alert engine. Generally, the central end maintains default; for the edge alert engine n9e-edge, you can customize the EngineName, such as edge1, edge2, etc. Alert engines with the same EngineName are considered a cluster.  Center # Unique configurations for the central n9e, not present in the edge alert engine n9e-edge. These are the unique configurations related to the old version of n9e-webapi.\n[Center] MetricsYamlFile = \u0026quot;./etc/metrics.yaml\u0026quot; I18NHeaderKey = \u0026quot;X-Language\u0026quot; [Center.AnonymousAccess] PromQuerier = true AlertDetail = true   MetricsYamlFile: Path to the metric configuration file. The explanations of metrics you see in the quick view come from this configuration file. Later, the metric view was launched, making this configuration file less important, and even the quick view function is planned to be removed. I18NHeader: This is a configuration item for developers; ordinary users don\u0026rsquo;t need to care about it. Center.AnonymousAccess: Configuration items related to anonymous access. PromQuerier indicates whether to allow anonymous query of interfaces of various data sources; AlertDetail indicates whether to allow anonymous viewing of alert details. It can be enabled in internal network environments but must be disabled in public network environments.  The dashboard has a public access function, which can even be set to be accessible without login, but this requires PromQuerier to be set to true. That is, if PromQuerier = false, even if the dashboard is set to public access, login is still required.\nPushgw # Although Nightingale does not directly store monitoring data, it provides multiple interfaces for receiving monitoring data, such as interfaces for the Prometheus remote write protocol, OpenTSDB protocol, etc. After receiving the data, Nightingale forwards the monitoring data to the backend time-series database, so Nightingale acts as a Pushgateway here, and the configuration items related to Pushgateway are under Pushgw.\n[Pushgw] # use target labels in database instead of in series LabelRewrite = true ForceUseServerTS = true   LabelRewrite: Nightingale has a machine management menu where you can tag machines, and these tags are attached to time-series data related to the machines. However, if a tag in the reported data conflicts with a tag in machine management, which one takes precedence? If LabelRewrite is true, the tag in machine management takes precedence; otherwise, the reported tag takes precedence. ForceUseServerTS: Whether to force the use of the server\u0026rsquo;s timestamp to overwrite the timestamp of the received monitoring data. Previously, there was no this configuration item. Due to confusion caused by uncalibrated machine time in many companies, Nightingale provides this configuration. It is recommended to enable it to uniformly use the server\u0026rsquo;s timestamp.  Pushgw.DebugSample # [Pushgw.DebugSample] ident = \u0026quot;xx\u0026quot; __name__ = \u0026quot;cpu_usage_active\u0026quot;  This configuration is for debugging and troubleshooting. It is actually a filter condition for monitoring metrics. If the metrics reported to Nightingale meet this filter condition, they will be printed to the log. Generally, no configuration is needed; it can be commented out.\nPushgw.WriterOpt # [Pushgw.WriterOpt] QueueMaxSize = 1000000 QueuePopSize = 1000 QueueNumber = 0  This part of the configuration is commented out by default because normally users don\u0026rsquo;t need to pay attention to it. If Nightingale receives too much data, which gets congested in memory and eventually leads to metric loss, you need to consider adjusting the configuration here.\nNightingale creates QueueNumber queues in memory. After receiving monitoring data, it puts the data into these queues. The default configuration of QueueNumber is 0, indicating that the specific number is not specified, and queues are created according to the number of CPU cores. The maximum capacity of each queue is QueueMaxSize, which defaults to 1000000, meaning each queue can store up to 1 million data entries.\nEach queue corresponds to a goroutine, which fetches QueuePopSize metrics from the queue each time. The default is 1000, meaning 1000 data entries are fetched from the queue each time and written to the backend time-series database as a batch. This takes full advantage of multi-core CPU performance. Therefore, the number of QueueNumber is essentially equal to the concurrency of writing to the backend time-series database.\nPushgw.Writers # Here configure the remote write addresses of the backend time-series databases. All time-series databases supporting the remote write protocol can be configured here. Generally, only one needs to be configured; if you want to write to multiple time-series databases simultaneously, you can configure multiple.\n[[Pushgw.Writers]] Url = \u0026quot;http://127.0.0.1:9090/api/v1/write\u0026quot; BasicAuthUser = \u0026quot;xx\u0026quot; BasicAuthPass = \u0026quot;xx\u0026quot; [[Pushgw.Writers]] Url = \u0026quot;http://127.0.0.1:8482/api/v1/write\u0026quot; BasicAuthUser = \u0026quot;xx\u0026quot; BasicAuthPass = \u0026quot;xx\u0026quot;   Url: Remote write address of the time-series database BasicAuth: If the time-series database requires BasicAuth authentication, configure the BasicAuth username and password here Headers: If the time-series database requires additional headers, configure them here Timeout: Write timeout in milliseconds DialTimeout: Connection timeout in milliseconds  Pushgw.Writers.WriteRelabels # Data written to the time-series database can undergo relabeling before writing. This configuration item is for relabeling, similar to Prometheus\u0026rsquo;s relabel configuration items, except that Prometheus uses yaml format while Nightingale uses toml format.\nIbex # Configuration items for the fault self-healing engine Ibex, i.e., the function for remote script execution. Originally, this function was a separate module called ibex, which was later merged into n9e, so this configuration item is also in n9e.\n[Ibex] Enable = true RPCListen = \u0026quot;0.0.0.0:20090\u0026quot;   Enable: Whether to enable the Ibex server function RPCListen: RPC service listening address of Ibex  n9e-edge configuration # The configuration file for the edge alert engine n9e-edge is etc/edge/edge.toml, and most configurations are the same as those of the central n9e. For more information, you can refer to this article: ã€ŠNightingale Monitoring - Edge Alert Engine Architecture Detailed Explanationã€‹.\n","description":"Explanation of the configuration files for Nightingale monitoring, with detailed descriptions of each configuration item","title":"Configuration"},{"RelPermalink":"/docs/agent/categraf/","contents":"Categraf is an agent which can collect metrics and logs. Categraf uses prometheus remote write as data push protocol, so it can push metrics to Nightingale.\nConfiguration # Configuration file of categraf: conf/config.toml\n[writer_opt] # default: 2000 batch = 2000 # channel(as queue) size chan_size = 10000 [[writers]] url = \u0026quot;http://N9E:17000/prometheus/v1/write\u0026quot; # Basic auth username basic_auth_user = \u0026quot;\u0026quot; # Basic auth password basic_auth_pass = \u0026quot;\u0026quot; # timeout settings, unit: ms timeout = 5000 dial_timeout = 2500 max_idle_conns_per_host = 100 [heartbeat] enable = true # report os version cpu.util mem.util metadata url = \u0026quot;http://N9E:17000/v1/n9e/heartbeat\u0026quot; # interval, unit: s interval = 10 # Basic auth username basic_auth_user = \u0026quot;\u0026quot; # Basic auth password basic_auth_pass = \u0026quot;\u0026quot; ## Optional headers # headers = [\u0026quot;X-From\u0026quot;, \u0026quot;categraf\u0026quot;, \u0026quot;X-Xyz\u0026quot;, \u0026quot;abc\u0026quot;] # timeout settings, unit: ms timeout = 5000 dial_timeout = 2500 max_idle_conns_per_host = 100  We highly recommend that you use Categraf as collector for Nightingale.\n","description":"Use Categraf as collector for Nightingale","title":"Categraf"},{"RelPermalink":"/docs/agent/telegraf/","contents":"Introduction # Telegraf is an agent for collecting, processing, aggregating, and writing metrics. Based on a plugin system to enable developers in the community to easily add support for additional metric collection.\nTelegraf supports multiple output plugins, we can use opentsdb or prometheusremotewrite plugin to send metrics to Nightingale. Below is an example configuration using opentsdb.\nInstall # #!/bin/sh version=1.20.4 tarball=telegraf-${version}_linux_amd64.tar.gz wget https://dl.influxdata.com/telegraf/releases/$tarball tar xzvf $tarball mkdir -p /opt/telegraf cp -far telegraf-${version}/usr/bin/telegraf /opt/telegraf cat \u0026lt;\u0026lt;EOF \u0026gt; /opt/telegraf/telegraf.conf [global_tags] [agent] interval = \u0026quot;10s\u0026quot; round_interval = true metric_batch_size = 1000 metric_buffer_limit = 10000 collection_jitter = \u0026quot;0s\u0026quot; flush_interval = \u0026quot;10s\u0026quot; flush_jitter = \u0026quot;0s\u0026quot; precision = \u0026quot;\u0026quot; hostname = \u0026quot;\u0026quot; omit_hostname = false [[outputs.opentsdb]] host = \u0026quot;http://127.0.0.1\u0026quot; port = 17000 http_batch_size = 50 http_path = \u0026quot;/opentsdb/put\u0026quot; debug = false separator = \u0026quot;_\u0026quot; [[inputs.cpu]] percpu = true totalcpu = true collect_cpu_time = false report_active = true [[inputs.disk]] ignore_fs = [\u0026quot;tmpfs\u0026quot;, \u0026quot;devtmpfs\u0026quot;, \u0026quot;devfs\u0026quot;, \u0026quot;iso9660\u0026quot;, \u0026quot;overlay\u0026quot;, \u0026quot;aufs\u0026quot;, \u0026quot;squashfs\u0026quot;] [[inputs.diskio]] [[inputs.kernel]] [[inputs.mem]] [[inputs.processes]] [[inputs.system]] fielddrop = [\u0026quot;uptime_format\u0026quot;] [[inputs.net]] ignore_protocol_stats = true EOF cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/systemd/system/telegraf.service [Unit] Description=\u0026quot;telegraf\u0026quot; After=network.target [Service] Type=simple ExecStart=/opt/telegraf/telegraf --config telegraf.conf WorkingDirectory=/opt/telegraf SuccessExitStatus=0 LimitNOFILE=65535 StandardOutput=syslog StandardError=syslog SyslogIdentifier=telegraf KillMode=process KillSignal=SIGQUIT TimeoutStopSec=5 Restart=always [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable telegraf systemctl restart telegraf systemctl status telegraf  ","description":"Use Telegraf as collector for Nightingale","title":"Telegraf"},{"RelPermalink":"/docs/agent/datadog-agent/","contents":"Configuration # Mofidy the configuration item dd_url in the file /etc/datadog-agent/datadog.yaml.\ndd_url: http://nightingale-address/datadog  nightingale-address is your Nightingale address.\nRestart # Restart the Datadog-Agent.\nsystemctl restart datadog-agent  ","description":"Use Datadog-Agent as collector for Nightingale","title":"Datadog-Agent"}]