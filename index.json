[{"RelPermalink":"/blog/say-hello-to-doks/","contents":"","description":"Introducing Doks, a Hugo theme helping you build modern documentation websites that are secure, fast, and SEO-ready â€” by default.","title":"Say hello to Doks ðŸ‘‹"},{"RelPermalink":"/docs/usage/datasource/","contents":"Nightingale supports various data sources, including:\n Prometheus, as well as other storage systems that support the Prometheus protocol, such as VictoriaMetrics and Thanos ElasticSearch, as well as other storage systems that support the ElasticSearch protocol, such as OpenSearch Grafana Loki  Add a data source in the Integrations-Data sources, select the corresponding data source type, fill in the data source address, username, password, and other information, and click Save.\n","description":null,"title":"Config datasource"},{"RelPermalink":"/docs/agent/intro/","contents":"Nightingale is an alarm engine, which does not need to be integrated with the collector, but directly connects to various data sources for query alarms.\nThat is to say, if you have collected various monitoring data and stored it in the time series database, you can configure the time series database as a data source in Nightingale, and Nightingale can directly query the data in the time series database. There is no need to use various collectors mentioned in this chapter.\nHowever, many new users have not built their own collection capabilities, so we provide some collector docking solutions to facilitate users to get started quickly. However, Nightingale still does not provide storage capabilities. These collectors collect data and push it to Nightingale, and Nightingale then forwards the data to the time series database.\nIn the Nightingale configuration file etc/config.toml, there is a [[Pushgw.Writers]] section, which is used to configure the address of the time series database. After receiving the data, Nightingale forwards the data to these addresses.\n","description":"","title":"Pre explanation"},{"RelPermalink":"/docs/prologue/introduction/","contents":"Nightingale is an open-source project focused on alerting. Similar to Grafana\u0026rsquo;s data source integration approach, Nightingale also integrates with various existing data sources. While Grafana focuses on visualization, Nightingale focuses on alerting.\nNightingale can query data from multiple data sources, generate alarm events, and then send alerts via various notification channels. In addition, we also have an event pipeline design, which can perform different processing on alarm events, such as filtering, relabeling, enriching, and so on.\nRepo #  Backend: https://github.com/ccfos/nightingale Frontend: https://github.com/n9e/fe  Any issues or PRs are welcome!\nWorking logic # Many users have already collected metrics and log data themselves. In this case, they can integrate their storage repositories (like VictoriaMetrics, Elasticsearch, etc.) as data sources in Nightingale. Users can then configure alert rules and notification rules in Nightingale to generate and dispatch alert events.\nNightingale itself does not provide data collection capabilities. We recommend using Categraf as a collector, which can seamlessly integrate with Nightingale.\nCategraf can collect monitoring data from operating systems, network devices, middleware, and databases. It pushes this data to Nightingale (via Prometheus Remote Write protocol), which then forwards the data to time-series databases (like Prometheus, VictoriaMetrics, etc.) and provides alerting and visualization capabilities.\nFor specific edge data centers, where the network link to the central Nightingale server is poor, Nightingale also provides a design for edge data center alerting engine deployment. In this mode, even if the edge and central networks are disconnected, alerting functionality remains unaffected.\n In the above diagram, the network link between data center A and the central data center is good, so the alerting engine is handled by the central Nightingale process. For data center B, where the network link to the central data center is poor, we deploy n9e-edge as the alerting engine to handle data source alerting functionality locally.\n Alerting, Upgrades, and Collaboration # Nightingale focuses on being an alerting engine, responsible for generating alert events and flexibly dispatching them based on rules. It has built-in support for 20 notification channels (like phone calls, SMS, email, DingTalk, Feishu, WeCom, Slack, etc.).\nIf you have more advanced requirements, such as:\n Want to aggregate events from multiple monitoring systems into one platform for unified noise reduction, response handling, and data analysis Want to support team on-call culture, including features like alert claim, escalation (to avoid missing alerts), and collaborative handling  Then Nightingale may not be suitable. We recommend using FlashDuty, an on-call product that aggregates alerts from various monitoring systems for unified noise reduction, distribution, and response.\nKey Capabilities #  Nightingale supports alert rules, muting rules, subscription rules, and notification rules. It natively integrates 20 notification channels and allows customization of message templates. Nightingale supports event pipelines to process alert events and integrate with third-party systems. For example, it can perform operations like relabeling, filtering, and enriching on events. Nightingale supports the concept of business groups and introduces a permission system to manage various rules in a categorized manner. Many databases and middleware have built-in alert rules that can be directly imported for use, and Prometheus alert rules can also be directly imported. Nightingale supports alert self-healing, which means that after an alert is triggered, a script is automatically executed to perform some predefined logic, such as cleaning up the disk or capturing the on-site situation.   Nightingale archives historical alert events and supports multi-dimensional querying and statistics. It allows flexible aggregation and grouping, making it easy to get a clear overview of the distribution of the company\u0026rsquo;s alert events at a glance.   Nightingale has built-in metric explanations, dashboards, and alert rules for common operating systems, middleware, and databases. However, these are all contributed by the community, and their overall quality varies. Nightingale directly receives data from multiple protocols such as Remote Write, OpenTSDB, Datadog, and Falcon, thus enabling integration with various types of agents. Nightingale supports multiple data sources including Prometheus, ElasticSearch, Loki, and TDEngine, and can perform alerting based on the data from them. Nightingale can be easily embedded into internal enterprise systems, such as Grafana and CMDB. It even allows configuring the menu visibility of these embedded systems.   Nightingale supports dashboard functionality, featuring common chart types and some built-in dashboards. The image above is a screenshot of one of these dashboards. If you\u0026rsquo;re already accustomed to Grafana, it\u0026rsquo;s recommended to continue using Grafana for viewing charts, as Grafana has more profound expertise in this area. For machine-related monitoring data collected by Categraf, it\u0026rsquo;s advisable to use Nightingale\u0026rsquo;s built-in dashboards. This is because Categraf\u0026rsquo;s metric naming follows Telegraf\u0026rsquo;s naming convention, which differs from that of Node Exporter. Since Nightingale incorporates the concept of business groups, where machines can belong to different business groups, there are times when you may only want to view machines belonging to the current business group in the dashboard. Therefore, Nightingale\u0026rsquo;s dashboards can be linked with business groups.  Thank you for the trust from numerous enterprises. # Nightingale has been adopted by many enterprises, including but not limited to:\nOpen Source License # The Nightingale monitoring project is open-sourced under the Apache License 2.0.\n","description":"Nightingale is an open-source project focused on alerting. Similar to Grafana's data source integration approach, Nightingale also connects with various existing data sources. However, while Grafana focuses on visualization, Nightingale emphasizes alerting engines.","title":"Introduction"},{"RelPermalink":"/docs/usage/ad-hoc/","contents":"Nightingale supports ad-hoc queries, which allow you to query data sources directly on the interface. You can query metric data in the Metrics-Explorer menu and log data in the Log Analysis-Explorer menu.\n","description":null,"title":"Ad-hoc Query"},{"RelPermalink":"/docs/usage/alert-rules/","contents":"Nightingale supports metric alerts and log alerts. According to the alert rules configured by the user, the data source is queried periodically. When the data in the data source meets the rule threshold, the alert is triggered.\nYou can manually create alert rules, import built-in alert rules, or import Prometheus alert rules.\n","description":null,"title":"Config alert rules"},{"RelPermalink":"/docs/prologue/prometheus/","contents":"Nightingale is similar to Grafana in that it can integrate with a variety of data sources, the most common of which is Prometheus-type. Other data sources that are compatible with the Prometheus interface, such as VictoriaMetrics, Thanos, and M3DB, can also be considered Prometheus-type sources, so the relationship between the two is close.\nIf you have the following requirements, you might consider using Nightingale:\n You have multiple time-series databases, such as Prometheus and VictoriaMetrics, and want to use a unified platform to manage various alert rules with permission control. You are concerned about the single point of failure of Prometheus\u0026rsquo;s alerting engine and want to avoid downtime. In addition to Prometheus alerts, you need alerts from other data sources such as ElasticSearch, Loki, and ClickHouse. You require more flexible alert rule configurations, such as controlling the effective time, event relabeling, event linkage with CMDB, and supporting alert self-healing scripts.  Nightingale also has visualization capabilities similar to Grafana, but it may not be as advanced. In my observation, many companies adopt a combination approach (in the adult world, there are no absolutes):\n Data Collection: A combination of various agents and exporters is used, with Categraf being the primary choice (especially for machine monitoring, seamlessly integrated with Nightingale), supplemented by various exporters. Storage: The time-series database primarily used is VictoriaMetrics, as it is compatible with Prometheus, offers better performance, and has a clustered version. For most companies, the single-node version is sufficient. Alerting Engine: Nightingale is used for alerting, making it easy for different teams to manage and collaborate. It comes with some built-in rules out of the box, and the configuration of alert rules is very flexible, with an event pipeline mechanism that facilitates integration with their own CMDB, etc. Visualization: Grafana is used for visualization, as it offers more advanced and visually appealing charts. The community is also very large, and many pre-made dashboards can be found on the Grafana site, making it relatively hassle-free. On-call Distribution of Alert Events: FlashDuty is used, which supports integration with various monitoring systems such as Zabbix, Prometheus, Nightingale, cloud monitoring solutions, Elastalert, etc. It consolidates alert events into a single platform for unified noise reduction, scheduling, claim escalation, response, distribution, and more.  ","description":"Nightingale and Prometheus are often discussed in relation to each other, and in fact, they have a complementary relationship. This article will detail the differences and connections between the two.","title":"Nightingale vs Prometheus"},{"RelPermalink":"/docs/install/compose/","contents":"$ git clone https://github.com/ccfos/nightingale.git $ cd nightingale/docker/compose-bridge $ docker-compose up -d Creating network \u0026quot;docker_nightingale\u0026quot; with driver \u0026quot;bridge\u0026quot; Creating mysql ... done Creating redis ... done Creating prometheus ... done Creating ibex ... done Creating agentd ... done Creating nwebapi ... done Creating nserver ... done Creating telegraf ... done $ docker-compose ps NAME IMAGE COMMAND SERVICE CREATED STATUS PORTS categraf flashcatcloud/categraf:latest \u0026quot;/entrypoint.sh\u0026quot; categraf 2 days ago Up 2 days ibex ulric2019/ibex:0.3 \u0026quot;sh -c '/wait \u0026amp;\u0026amp; /apâ€¦\u0026quot; ibex 2 days ago Up 2 days mysql mysql:5.7 \u0026quot;docker-entrypoint.sâ€¦\u0026quot; mysql 2 days ago Up 2 days n9e flashcatcloud/nightingale:latest \u0026quot;sh -c '/wait \u0026amp;\u0026amp; /apâ€¦\u0026quot; n9e 2 days ago Up 2 days prometheus prom/prometheus \u0026quot;/bin/prometheus --câ€¦\u0026quot; prometheus 2 days ago Up 2 days redis redis:6.2 \u0026quot;docker-entrypoint.sâ€¦\u0026quot; redis 2 days ago Up 2 days  Use your browser to access Nightingale\u0026rsquo;s web page at http://localhost:17000. The default username is root and default password is root.2020.\n","description":"Deploy nightingale via Docker compose","title":"Docker Compose"},{"RelPermalink":"/docs/install/helm/","contents":"You can use the n9e helm chart to run n9e in K8s cluster.\nThe default n9e username is root and password is root.2020.\n","description":"Nightingale Helm chart","title":"Helm"},{"RelPermalink":"/docs/install/binary/","contents":"Download # Download the latest release from GitHub, then you will get a tarball which name is like n9e-{version}-linux-amd64.tar.gz\nInstall using sqlite and miniredis # This approach doesn\u0026rsquo;t rely on MySQL and Redis. You can simply start it using the n9e binary. However, this method isn\u0026rsquo;t suitable for production environments and is only intended for testing purposes.\n#!/bin/bash mkdir /opt/n9e \u0026amp;\u0026amp; tar zxvf n9e-{version}-linux-amd64.tar.gz -C /opt/n9e cd /opt/n9e # check configurations in /opt/n9e/etc/config.toml and start n9e nohup ./n9e \u0026amp;\u0026gt; n9e.log \u0026amp;  Check Process # # check process is runing or not ss -tlnp|grep 17000  Login # Open web browser and go to http://localhost:17000. The default username is root and default password is root.2020.\n Please replace localhost with your server\u0026rsquo;s IP address.\n Install using MySQL and Redis # In production environments, we recommend using MySQL and Redis to store data. Modify the /opt/n9e/etc/config.toml configuration file to configure the connection information for MySQL and Redis.\nDB section:\n[DB] DBType = \u0026quot;mysql\u0026quot; DSN=\u0026quot;YourUsername:YourPassword@tcp(127.0.0.1:3306)/n9e_v6?charset=utf8mb4\u0026amp;parseTime=True\u0026amp;loc=Local\u0026quot;  Redis section:\n[Redis] Address = \u0026quot;127.0.0.1:6379\u0026quot; Password = \u0026quot;YourRedisPassword\u0026quot; RedisType = \u0026quot;standalone\u0026quot;  Start the n9e binary, and Nightingale will automatically create the database tables. Of course, your DB connection account needs to have the permission to create database tables.\nnohup ./n9e \u0026amp;\u0026gt; n9e.log \u0026amp;  Open web browser and go to http://localhost:17000. The default username is root and default password is root.2020.\n Please replace localhost with your server\u0026rsquo;s IP address.\n We recommend using systemd to manage the n9e process in production environments and set it to start automatically at boot time.\n","description":"Deploy nightingale via binary","title":"Binary install"},{"RelPermalink":"/docs/agent/categraf/","contents":"Categraf is an agent which can collect metrics and logs. Categraf uses prometheus remote write as data push protocol, so it can push metrics to Nightingale.\nConfiguration # Configuration file of categraf: conf/config.toml\n[writer_opt] # default: 2000 batch = 2000 # channel(as queue) size chan_size = 10000 [[writers]] url = \u0026quot;http://N9E:17000/prometheus/v1/write\u0026quot; # Basic auth username basic_auth_user = \u0026quot;\u0026quot; # Basic auth password basic_auth_pass = \u0026quot;\u0026quot; # timeout settings, unit: ms timeout = 5000 dial_timeout = 2500 max_idle_conns_per_host = 100 [heartbeat] enable = true # report os version cpu.util mem.util metadata url = \u0026quot;http://N9E:17000/v1/n9e/heartbeat\u0026quot; # interval, unit: s interval = 10 # Basic auth username basic_auth_user = \u0026quot;\u0026quot; # Basic auth password basic_auth_pass = \u0026quot;\u0026quot; ## Optional headers # headers = [\u0026quot;X-From\u0026quot;, \u0026quot;categraf\u0026quot;, \u0026quot;X-Xyz\u0026quot;, \u0026quot;abc\u0026quot;] # timeout settings, unit: ms timeout = 5000 dial_timeout = 2500 max_idle_conns_per_host = 100  We highly recommend that you use Categraf as collector for Nightingale.\n","description":"Use Categraf as collector for Nightingale","title":"Categraf"},{"RelPermalink":"/docs/agent/telegraf/","contents":"Introduction # Telegraf is an agent for collecting, processing, aggregating, and writing metrics. Based on a plugin system to enable developers in the community to easily add support for additional metric collection.\nTelegraf supports multiple output plugins, we can use opentsdb or prometheusremotewrite plugin to send metrics to Nightingale. Below is an example configuration using opentsdb.\nInstall # #!/bin/sh version=1.20.4 tarball=telegraf-${version}_linux_amd64.tar.gz wget https://dl.influxdata.com/telegraf/releases/$tarball tar xzvf $tarball mkdir -p /opt/telegraf cp -far telegraf-${version}/usr/bin/telegraf /opt/telegraf cat \u0026lt;\u0026lt;EOF \u0026gt; /opt/telegraf/telegraf.conf [global_tags] [agent] interval = \u0026quot;10s\u0026quot; round_interval = true metric_batch_size = 1000 metric_buffer_limit = 10000 collection_jitter = \u0026quot;0s\u0026quot; flush_interval = \u0026quot;10s\u0026quot; flush_jitter = \u0026quot;0s\u0026quot; precision = \u0026quot;\u0026quot; hostname = \u0026quot;\u0026quot; omit_hostname = false [[outputs.opentsdb]] host = \u0026quot;http://127.0.0.1\u0026quot; port = 17000 http_batch_size = 50 http_path = \u0026quot;/opentsdb/put\u0026quot; debug = false separator = \u0026quot;_\u0026quot; [[inputs.cpu]] percpu = true totalcpu = true collect_cpu_time = false report_active = true [[inputs.disk]] ignore_fs = [\u0026quot;tmpfs\u0026quot;, \u0026quot;devtmpfs\u0026quot;, \u0026quot;devfs\u0026quot;, \u0026quot;iso9660\u0026quot;, \u0026quot;overlay\u0026quot;, \u0026quot;aufs\u0026quot;, \u0026quot;squashfs\u0026quot;] [[inputs.diskio]] [[inputs.kernel]] [[inputs.mem]] [[inputs.processes]] [[inputs.system]] fielddrop = [\u0026quot;uptime_format\u0026quot;] [[inputs.net]] ignore_protocol_stats = true EOF cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/systemd/system/telegraf.service [Unit] Description=\u0026quot;telegraf\u0026quot; After=network.target [Service] Type=simple ExecStart=/opt/telegraf/telegraf --config telegraf.conf WorkingDirectory=/opt/telegraf SuccessExitStatus=0 LimitNOFILE=65535 StandardOutput=syslog StandardError=syslog SyslogIdentifier=telegraf KillMode=process KillSignal=SIGQUIT TimeoutStopSec=5 Restart=always [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable telegraf systemctl restart telegraf systemctl status telegraf  ","description":"Use Telegraf as collector for Nightingale","title":"Telegraf"},{"RelPermalink":"/docs/agent/datadog-agent/","contents":"Configuration # Mofidy the configuration item dd_url in the file /etc/datadog-agent/datadog.yaml.\ndd_url: http://nightingale-address/datadog  nightingale-address is your Nightingale address.\nRestart # Restart the Datadog-Agent.\nsystemctl restart datadog-agent  ","description":"Use Datadog-Agent as collector for Nightingale","title":"Datadog-Agent"}]