[{"RelPermalink":"/blog/say-hello-to-doks/","contents":"","description":"Introducing Doks, a Hugo theme helping you build modern documentation websites that are secure, fast, and SEO-ready â€” by default.","title":"Say hello to Doks ðŸ‘‹"},{"RelPermalink":"/docs/usage/datasource/","contents":"Nightingale supports various data sources, including:\n Prometheus, as well as other storage systems that support the Prometheus protocol, such as VictoriaMetrics and Thanos ElasticSearch, as well as other storage systems that support the ElasticSearch protocol, such as OpenSearch Grafana Loki  Add a data source in the Integrations-Data sources, select the corresponding data source type, fill in the data source address, username, password, and other information, and click Save.\n","description":null,"title":"Config datasource"},{"RelPermalink":"/docs/agent/intro/","contents":"Nightingale is an alarm engine, which does not need to be integrated with the collector, but directly connects to various data sources for query alarms.\nThat is to say, if you have collected various monitoring data and stored it in the time series database, you can configure the time series database as a data source in Nightingale, and Nightingale can directly query the data in the time series database. There is no need to use various collectors mentioned in this chapter.\nHowever, many new users have not built their own collection capabilities, so we provide some collector docking solutions to facilitate users to get started quickly. However, Nightingale still does not provide storage capabilities. These collectors collect data and push it to Nightingale, and Nightingale then forwards the data to the time series database.\nIn the Nightingale configuration file etc/config.toml, there is a [[Pushgw.Writers]] section, which is used to configure the address of the time series database. After receiving the data, Nightingale forwards the data to these addresses.\n","description":"","title":"Pre explanation"},{"RelPermalink":"/docs/prologue/introduction/","contents":"Nightingale is an open-source project focused on alerting. Similar to Grafana\u0026rsquo;s data source integration approach, Nightingale also integrates with various existing data sources. While Grafana focuses on visualization, Nightingale focuses on alerting.\nNightingale can query data from multiple data sources, generate alarm events, and then send alerts via various notification channels. In addition, we also have an event pipeline design, which can perform different processing on alarm events, such as filtering, relabeling, enriching, and so on.\nRepo #  Backend: https://github.com/ccfos/nightingale Frontend: https://github.com/n9e/fe  Any issues or PRs are welcome!\nArchitecture # Nightingale can integrate with various data sources such as Prometheus, VictoriaMetrics, Elasticsearch, and Loki. It queries metrics and logs based on the alert rules configured by users, makes alert determinations, and then generates alert events, which are pushed to various notification channels.\nKey Capabilities # Nightingale enables flexible alarm configuration. It supports both metric and log data sources. Users can configure aspects such as the active time periods of alarm rules, the clusters in which the rules are effective, and event relabeling.\nAlthough Nightingale\u0026rsquo;s visualization capabilities are not as strong as those of Grafana, it still supports common dashboard chart types. Moreover, it has built-in alarm rules and dashboards for various middleware and databases, making it ready-to-use.\n","description":"Nightingale is an open-source project focused on alerting. Similar to Grafana's data source integration approach, Nightingale also connects with various existing data sources. However, while Grafana focuses on visualization, Nightingale emphasizes alerting engines.","title":"Introduction"},{"RelPermalink":"/docs/usage/ad-hoc/","contents":"Nightingale supports ad-hoc queries, which allow you to query data sources directly on the interface. You can query metric data in the Metrics-Explorer menu and log data in the Log Analysis-Explorer menu.\n","description":null,"title":"Ad-hoc Query"},{"RelPermalink":"/docs/usage/alert-rules/","contents":"Nightingale supports metric alerts and log alerts. According to the alert rules configured by the user, the data source is queried periodically. When the data in the data source meets the rule threshold, the alert is triggered.\nYou can manually create alert rules, import built-in alert rules, or import Prometheus alert rules.\n","description":null,"title":"Config alert rules"},{"RelPermalink":"/docs/install/compose/","contents":"$ git clone https://github.com/ccfos/nightingale.git $ cd nightingale/docker/compose-bridge $ docker-compose up -d Creating network \u0026quot;docker_nightingale\u0026quot; with driver \u0026quot;bridge\u0026quot; Creating mysql ... done Creating redis ... done Creating prometheus ... done Creating ibex ... done Creating agentd ... done Creating nwebapi ... done Creating nserver ... done Creating telegraf ... done $ docker-compose ps NAME IMAGE COMMAND SERVICE CREATED STATUS PORTS categraf flashcatcloud/categraf:latest \u0026quot;/entrypoint.sh\u0026quot; categraf 2 days ago Up 2 days ibex ulric2019/ibex:0.3 \u0026quot;sh -c '/wait \u0026amp;\u0026amp; /apâ€¦\u0026quot; ibex 2 days ago Up 2 days mysql mysql:5.7 \u0026quot;docker-entrypoint.sâ€¦\u0026quot; mysql 2 days ago Up 2 days n9e flashcatcloud/nightingale:latest \u0026quot;sh -c '/wait \u0026amp;\u0026amp; /apâ€¦\u0026quot; n9e 2 days ago Up 2 days prometheus prom/prometheus \u0026quot;/bin/prometheus --câ€¦\u0026quot; prometheus 2 days ago Up 2 days redis redis:6.2 \u0026quot;docker-entrypoint.sâ€¦\u0026quot; redis 2 days ago Up 2 days  Use your browser to access Nightingale\u0026rsquo;s web page at http://localhost:17000. The default username is root and default password is root.2020.\n","description":"Deploy nightingale via Docker compose","title":"Docker Compose"},{"RelPermalink":"/docs/install/helm/","contents":"You can use the n9e helm chart to run n9e in K8s cluster.\nThe default n9e username is root and password is root.2020.\n","description":"Nightingale Helm chart","title":"Helm"},{"RelPermalink":"/docs/install/binary/","contents":"Download # Download the latest release from GitHub, then you will get a tarball which name is like n9e-{version}-linux-amd64.tar.gz\nInstall using sqlite and miniredis # This approach doesn\u0026rsquo;t rely on MySQL and Redis. You can simply start it using the n9e binary. However, this method isn\u0026rsquo;t suitable for production environments and is only intended for testing purposes.\n#!/bin/bash mkdir /opt/n9e \u0026amp;\u0026amp; tar zxvf n9e-{version}-linux-amd64.tar.gz -C /opt/n9e cd /opt/n9e # check configurations in /opt/n9e/etc/config.toml and start n9e nohup ./n9e \u0026amp;\u0026gt; n9e.log \u0026amp;  Check Process # # check process is runing or not ss -tlnp|grep 17000  Login # Open web browser and go to http://localhost:17000. The default username is root and default password is root.2020.\n Please replace localhost with your server\u0026rsquo;s IP address.\n Install using MySQL and Redis # In production environments, we recommend using MySQL and Redis to store data. Modify the /opt/n9e/etc/config.toml configuration file to configure the connection information for MySQL and Redis.\nDB section:\n[DB] DBType = \u0026quot;mysql\u0026quot; DSN=\u0026quot;YourUsername:YourPassword@tcp(127.0.0.1:3306)/n9e_v6?charset=utf8mb4\u0026amp;parseTime=True\u0026amp;loc=Local\u0026quot;  Redis section:\n[Redis] Address = \u0026quot;127.0.0.1:6379\u0026quot; Password = \u0026quot;YourRedisPassword\u0026quot; RedisType = \u0026quot;standalone\u0026quot;  Start the n9e binary, and Nightingale will automatically create the database tables. Of course, your DB connection account needs to have the permission to create database tables.\nnohup ./n9e \u0026amp;\u0026gt; n9e.log \u0026amp;  Open web browser and go to http://localhost:17000. The default username is root and default password is root.2020.\n Please replace localhost with your server\u0026rsquo;s IP address.\n We recommend using systemd to manage the n9e process in production environments and set it to start automatically at boot time.\n","description":"Deploy nightingale via binary","title":"Binary install"},{"RelPermalink":"/docs/agent/categraf/","contents":"Categraf is an agent which can collect metrics and logs. Categraf uses prometheus remote write as data push protocol, so it can push metrics to Nightingale.\nConfiguration # Configuration file of categraf: conf/config.toml\n[writer_opt] # default: 2000 batch = 2000 # channel(as queue) size chan_size = 10000 [[writers]] url = \u0026quot;http://N9E:17000/prometheus/v1/write\u0026quot; # Basic auth username basic_auth_user = \u0026quot;\u0026quot; # Basic auth password basic_auth_pass = \u0026quot;\u0026quot; # timeout settings, unit: ms timeout = 5000 dial_timeout = 2500 max_idle_conns_per_host = 100 [heartbeat] enable = true # report os version cpu.util mem.util metadata url = \u0026quot;http://N9E:17000/v1/n9e/heartbeat\u0026quot; # interval, unit: s interval = 10 # Basic auth username basic_auth_user = \u0026quot;\u0026quot; # Basic auth password basic_auth_pass = \u0026quot;\u0026quot; ## Optional headers # headers = [\u0026quot;X-From\u0026quot;, \u0026quot;categraf\u0026quot;, \u0026quot;X-Xyz\u0026quot;, \u0026quot;abc\u0026quot;] # timeout settings, unit: ms timeout = 5000 dial_timeout = 2500 max_idle_conns_per_host = 100  We highly recommend that you use Categraf as collector for Nightingale.\n","description":"Use Categraf as collector for Nightingale","title":"Categraf"},{"RelPermalink":"/docs/agent/telegraf/","contents":"Introduction # Telegraf is an agent for collecting, processing, aggregating, and writing metrics. Based on a plugin system to enable developers in the community to easily add support for additional metric collection.\nTelegraf supports multiple output plugins, we can use opentsdb or prometheusremotewrite plugin to send metrics to Nightingale. Below is an example configuration using opentsdb.\nInstall # #!/bin/sh version=1.20.4 tarball=telegraf-${version}_linux_amd64.tar.gz wget https://dl.influxdata.com/telegraf/releases/$tarball tar xzvf $tarball mkdir -p /opt/telegraf cp -far telegraf-${version}/usr/bin/telegraf /opt/telegraf cat \u0026lt;\u0026lt;EOF \u0026gt; /opt/telegraf/telegraf.conf [global_tags] [agent] interval = \u0026quot;10s\u0026quot; round_interval = true metric_batch_size = 1000 metric_buffer_limit = 10000 collection_jitter = \u0026quot;0s\u0026quot; flush_interval = \u0026quot;10s\u0026quot; flush_jitter = \u0026quot;0s\u0026quot; precision = \u0026quot;\u0026quot; hostname = \u0026quot;\u0026quot; omit_hostname = false [[outputs.opentsdb]] host = \u0026quot;http://127.0.0.1\u0026quot; port = 17000 http_batch_size = 50 http_path = \u0026quot;/opentsdb/put\u0026quot; debug = false separator = \u0026quot;_\u0026quot; [[inputs.cpu]] percpu = true totalcpu = true collect_cpu_time = false report_active = true [[inputs.disk]] ignore_fs = [\u0026quot;tmpfs\u0026quot;, \u0026quot;devtmpfs\u0026quot;, \u0026quot;devfs\u0026quot;, \u0026quot;iso9660\u0026quot;, \u0026quot;overlay\u0026quot;, \u0026quot;aufs\u0026quot;, \u0026quot;squashfs\u0026quot;] [[inputs.diskio]] [[inputs.kernel]] [[inputs.mem]] [[inputs.processes]] [[inputs.system]] fielddrop = [\u0026quot;uptime_format\u0026quot;] [[inputs.net]] ignore_protocol_stats = true EOF cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/systemd/system/telegraf.service [Unit] Description=\u0026quot;telegraf\u0026quot; After=network.target [Service] Type=simple ExecStart=/opt/telegraf/telegraf --config telegraf.conf WorkingDirectory=/opt/telegraf SuccessExitStatus=0 LimitNOFILE=65535 StandardOutput=syslog StandardError=syslog SyslogIdentifier=telegraf KillMode=process KillSignal=SIGQUIT TimeoutStopSec=5 Restart=always [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable telegraf systemctl restart telegraf systemctl status telegraf  ","description":"Use Telegraf as collector for Nightingale","title":"Telegraf"},{"RelPermalink":"/docs/agent/datadog-agent/","contents":"Configuration # Mofidy the configuration item dd_url in the file /etc/datadog-agent/datadog.yaml.\ndd_url: http://nightingale-address/datadog  nightingale-address is your Nightingale address.\nRestart # Restart the Datadog-Agent.\nsystemctl restart datadog-agent  ","description":"Use Datadog-Agent as collector for Nightingale","title":"Datadog-Agent"}]